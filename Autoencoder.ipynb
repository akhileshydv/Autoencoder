{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Autoencoder.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "cg-DbOBZN_5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d6529eb-4f81-4294-ca46-167af3815b5f"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ws_ycW_tOGEQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a2c377a-baa5-42de-9593-602b15fc9f2f"
      },
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras.backend as K\n",
        "\n",
        "from time import time\n",
        "from sklearn.cluster import KMeans\n",
        "from keras import callbacks\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense, Input\n",
        "from keras.initializers import VarianceScaling\n",
        "from keras.engine.topology import Layer, InputSpec\n",
        "\n",
        "from scipy.misc import imread\n",
        "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kyK6Yt0uOOV8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "28e4d4aa-08e4-497d-9e39-e41da00a2351"
      },
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (val_x, val_y) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 1s 24us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 28s 1us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 29s 7us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CvtBFoIZO45w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_x = train_x/255.\n",
        "val_x = val_x/255.\n",
        "\n",
        "train_x = train_x.reshape(-1, 784)\n",
        "val_x = val_x.reshape(-1, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WOqqBWbVOVN-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    # this is our input placeholder\n",
        "    input_img = Input(shape=(784,))\n",
        "\n",
        "    # \"encoded\" is the encoded representation of the input\n",
        "    encoded = Dense(2000, activation='relu')(input_img)\n",
        "    encoded = Dense(500, activation='relu')(encoded)\n",
        "    encoded = Dense(500, activation='relu')(encoded)\n",
        "    encoded = Dense(10, activation='sigmoid')(encoded)\n",
        "\n",
        "    # \"decoded\" is the lossy reconstruction of the input\n",
        "    decoded = Dense(500, activation='relu')(encoded)\n",
        "    decoded = Dense(500, activation='relu')(decoded)\n",
        "    decoded = Dense(2000, activation='relu')(decoded)\n",
        "    decoded = Dense(784)(decoded)\n",
        "\n",
        "    # this model maps an input to its reconstruction\n",
        "    autoencoder = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rhptPcXMOcKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "bf081f0e-bccc-4f94-c9b1-3eff830793c6"
      },
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2000)              1570000   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 500)               1000500   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                5010      \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 500)               5500      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 500)               250500    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 2000)              1002000   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 784)               1568784   \n",
            "=================================================================\n",
            "Total params: 5,652,794\n",
            "Trainable params: 5,652,794\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lol3tb8HPQUL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#  this model maps an input to its encoded representation\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4jpLleZcPhxF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adam', loss='mse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0oXz8QRVPidu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HdhD9vZYPnW_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6239
        },
        "outputId": "7fbd0d14-05d1-4ee8-ee3a-810996661fb3"
      },
      "cell_type": "code",
      "source": [
        "with tf.device('/gpu:0'):\n",
        "   train_history = autoencoder.fit(train_x, train_x, epochs=500, batch_size=2048, validation_data=(val_x, val_x), callbacks=[estop])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/500\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.0921 - val_loss: 0.0685\n",
            "Epoch 2/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0588 - val_loss: 0.0480\n",
            "Epoch 3/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0429 - val_loss: 0.0392\n",
            "Epoch 4/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0370 - val_loss: 0.0347\n",
            "Epoch 5/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0329 - val_loss: 0.0304\n",
            "Epoch 6/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0295 - val_loss: 0.0274\n",
            "Epoch 7/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0263 - val_loss: 0.0253\n",
            "Epoch 8/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0251 - val_loss: 0.0244\n",
            "Epoch 9/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0238 - val_loss: 0.0236\n",
            "Epoch 10/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0229 - val_loss: 0.0229\n",
            "Epoch 11/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0230 - val_loss: 0.0219\n",
            "Epoch 12/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0212 - val_loss: 0.0210\n",
            "Epoch 13/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0209 - val_loss: 0.0223\n",
            "Epoch 14/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0203 - val_loss: 0.0196\n",
            "Epoch 15/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0192 - val_loss: 0.0190\n",
            "Epoch 16/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0187 - val_loss: 0.0187\n",
            "Epoch 17/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0188 - val_loss: 0.0183\n",
            "Epoch 18/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0179 - val_loss: 0.0191\n",
            "Epoch 19/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0178 - val_loss: 0.0174\n",
            "Epoch 20/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0171 - val_loss: 0.0173\n",
            "Epoch 21/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0173 - val_loss: 0.0169\n",
            "Epoch 22/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0164 - val_loss: 0.0163\n",
            "Epoch 23/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0162 - val_loss: 0.0160\n",
            "Epoch 24/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0162 - val_loss: 0.0162\n",
            "Epoch 25/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0156 - val_loss: 0.0156\n",
            "Epoch 26/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0156 - val_loss: 0.0155\n",
            "Epoch 27/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0151 - val_loss: 0.0151\n",
            "Epoch 28/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0151 - val_loss: 0.0153\n",
            "Epoch 29/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0147 - val_loss: 0.0148\n",
            "Epoch 30/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0147 - val_loss: 0.0159\n",
            "Epoch 31/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0146 - val_loss: 0.0146\n",
            "Epoch 32/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0143 - val_loss: 0.0144\n",
            "Epoch 33/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0142 - val_loss: 0.0145\n",
            "Epoch 34/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0141 - val_loss: 0.0142\n",
            "Epoch 35/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0143 - val_loss: 0.0143\n",
            "Epoch 36/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0139 - val_loss: 0.0140\n",
            "Epoch 37/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0138 - val_loss: 0.0140\n",
            "Epoch 38/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0137 - val_loss: 0.0139\n",
            "Epoch 39/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0140 - val_loss: 0.0138\n",
            "Epoch 40/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0135 - val_loss: 0.0143\n",
            "Epoch 41/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0136 - val_loss: 0.0136\n",
            "Epoch 42/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0133 - val_loss: 0.0143\n",
            "Epoch 43/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0136 - val_loss: 0.0135\n",
            "Epoch 44/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0132 - val_loss: 0.0136\n",
            "Epoch 45/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0134 - val_loss: 0.0135\n",
            "Epoch 46/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0131 - val_loss: 0.0133\n",
            "Epoch 47/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0132 - val_loss: 0.0136\n",
            "Epoch 48/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0130 - val_loss: 0.0133\n",
            "Epoch 49/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0131 - val_loss: 0.0137\n",
            "Epoch 50/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0130 - val_loss: 0.0131\n",
            "Epoch 51/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0129 - val_loss: 0.0131\n",
            "Epoch 52/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0128 - val_loss: 0.0131\n",
            "Epoch 53/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0128 - val_loss: 0.0133\n",
            "Epoch 54/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0127 - val_loss: 0.0130\n",
            "Epoch 55/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0127 - val_loss: 0.0130\n",
            "Epoch 56/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0128 - val_loss: 0.0129\n",
            "Epoch 57/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0124 - val_loss: 0.0130\n",
            "Epoch 58/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0126 - val_loss: 0.0130\n",
            "Epoch 59/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0124 - val_loss: 0.0127\n",
            "Epoch 60/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0124 - val_loss: 0.0129\n",
            "Epoch 61/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0124 - val_loss: 0.0126\n",
            "Epoch 62/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0123 - val_loss: 0.0131\n",
            "Epoch 63/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0125 - val_loss: 0.0128\n",
            "Epoch 64/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0125\n",
            "Epoch 65/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0126\n",
            "Epoch 66/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0126\n",
            "Epoch 67/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0126\n",
            "Epoch 68/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0124\n",
            "Epoch 69/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0119 - val_loss: 0.0125\n",
            "Epoch 70/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0120 - val_loss: 0.0130\n",
            "Epoch 71/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0121 - val_loss: 0.0123\n",
            "Epoch 72/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0119 - val_loss: 0.0123\n",
            "Epoch 73/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0118 - val_loss: 0.0123\n",
            "Epoch 74/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0119 - val_loss: 0.0123\n",
            "Epoch 75/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 76/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0118 - val_loss: 0.0122\n",
            "Epoch 77/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0118 - val_loss: 0.0125\n",
            "Epoch 78/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0117 - val_loss: 0.0122\n",
            "Epoch 79/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 80/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0120 - val_loss: 0.0122\n",
            "Epoch 81/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0116 - val_loss: 0.0121\n",
            "Epoch 82/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0116 - val_loss: 0.0123\n",
            "Epoch 83/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0116 - val_loss: 0.0121\n",
            "Epoch 84/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0120\n",
            "Epoch 85/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0116 - val_loss: 0.0124\n",
            "Epoch 86/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0115 - val_loss: 0.0125\n",
            "Epoch 87/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0120\n",
            "Epoch 88/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0120\n",
            "Epoch 89/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0115 - val_loss: 0.0121\n",
            "Epoch 90/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0120\n",
            "Epoch 91/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 92/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0114 - val_loss: 0.0121\n",
            "Epoch 93/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 94/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 95/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0113 - val_loss: 0.0122\n",
            "Epoch 96/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0113 - val_loss: 0.0119\n",
            "Epoch 97/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0112 - val_loss: 0.0119\n",
            "Epoch 98/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0114 - val_loss: 0.0119\n",
            "Epoch 99/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0112 - val_loss: 0.0118\n",
            "Epoch 100/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0112 - val_loss: 0.0118\n",
            "Epoch 101/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0112 - val_loss: 0.0118\n",
            "Epoch 102/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 103/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0119\n",
            "Epoch 104/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0118\n",
            "Epoch 105/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 106/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0112 - val_loss: 0.0120\n",
            "Epoch 107/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 108/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0117\n",
            "Epoch 109/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0112 - val_loss: 0.0117\n",
            "Epoch 110/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0118\n",
            "Epoch 111/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0117\n",
            "Epoch 112/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0124\n",
            "Epoch 113/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0111 - val_loss: 0.0117\n",
            "Epoch 114/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0116\n",
            "Epoch 115/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0116\n",
            "Epoch 116/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0117\n",
            "Epoch 117/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0108 - val_loss: 0.0116\n",
            "Epoch 118/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0110 - val_loss: 0.0117\n",
            "Epoch 119/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0116\n",
            "Epoch 120/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0120\n",
            "Epoch 121/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0116\n",
            "Epoch 122/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0116\n",
            "Epoch 123/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0109 - val_loss: 0.0116\n",
            "Epoch 124/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0115\n",
            "Epoch 125/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0115\n",
            "Epoch 126/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0108 - val_loss: 0.0116\n",
            "Epoch 127/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0117\n",
            "Epoch 128/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0115\n",
            "Epoch 129/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0121\n",
            "Epoch 130/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0108 - val_loss: 0.0115\n",
            "Epoch 131/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0117\n",
            "Epoch 132/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0115\n",
            "Epoch 133/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0115\n",
            "Epoch 134/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0114\n",
            "Epoch 135/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 136/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0107 - val_loss: 0.0115\n",
            "Epoch 137/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0115\n",
            "Epoch 138/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0115\n",
            "Epoch 139/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0115\n",
            "Epoch 140/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0106 - val_loss: 0.0115\n",
            "Epoch 141/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0115\n",
            "Epoch 142/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0114\n",
            "Epoch 143/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 144/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0116\n",
            "Epoch 145/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0114\n",
            "Epoch 146/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 147/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0105 - val_loss: 0.0114\n",
            "Epoch 148/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 149/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 150/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0114\n",
            "Epoch 151/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0116\n",
            "Epoch 152/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 153/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0115\n",
            "Epoch 154/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 155/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0116\n",
            "Epoch 156/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0114\n",
            "Epoch 157/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0113\n",
            "Epoch 158/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0118\n",
            "Epoch 159/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0104 - val_loss: 0.0114\n",
            "Epoch 160/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0115\n",
            "Epoch 161/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 162/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 163/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 164/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0115\n",
            "Epoch 165/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0113\n",
            "Epoch 166/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 167/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 168/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0102 - val_loss: 0.0114\n",
            "Epoch 169/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 170/500\n",
            "60000/60000 [==============================] - 2s 34us/step - loss: 0.0101 - val_loss: 0.0113\n",
            "Epoch 171/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 172/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0101 - val_loss: 0.0113\n",
            "Epoch 173/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0103 - val_loss: 0.0114\n",
            "Epoch 174/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0101 - val_loss: 0.0113\n",
            "Epoch 175/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 176/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0100 - val_loss: 0.0113\n",
            "Epoch 177/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0101 - val_loss: 0.0113\n",
            "Epoch 178/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0101 - val_loss: 0.0113\n",
            "Epoch 179/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0100 - val_loss: 0.0113\n",
            "Epoch 180/500\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0100 - val_loss: 0.0113\n",
            "Epoch 181/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0102 - val_loss: 0.0113\n",
            "Epoch 182/500\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.0100 - val_loss: 0.0113\n",
            "Epoch 00182: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_kl1NRCNPtFz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pred = autoencoder.predict(val_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v1eki3okSxp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9b07ec26-8e52-4676-e161-6805c1ce302f"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(pred[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3377eefc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyE8NapSMAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f339e7b8fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1SWt5D8MS0FE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "7c4abf55-2a50-4d3f-a615-3bde41866893"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(val_x[0].reshape(28, 28), cmap='gray')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3375e86898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCCjoIhFQ0EUioKCLREBBF4nA/wNtYzDYm8UacwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3377f48390>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3DKGp-nMUbtY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}